{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/geetua/lm-hackers/blob/main/lm-hackers.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import os\n",
        "from google.colab import userdata\n",
        "import openai\n",
        "\n",
        "# Set your OpenAI API key\n",
        "os.environ[\"OPENAI_API_KEY\"] = userdata.get('OPENAI_API_KEY')\n",
        "openai.api_key = os.getenv(\"OPENAI_API_KEY\")"
      ],
      "metadata": {
        "id": "bvpQCbDl3NkQ"
      },
      "id": "bvpQCbDl3NkQ",
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "id": "42568396-132f-422c-961b-5aaa03b3537d",
      "metadata": {
        "id": "42568396-132f-422c-961b-5aaa03b3537d"
      },
      "outputs": [],
      "source": [
        "import tokenize, ast\n",
        "from io import BytesIO"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "25344da5-3c07-457e-842a-7341d33c023c",
      "metadata": {
        "id": "25344da5-3c07-457e-842a-7341d33c023c"
      },
      "source": [
        "# A hacker's guide to Language Models"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0b017bfc-5be0-4e41-9fa1-9f685c3b0de5",
      "metadata": {
        "jp-MarkdownHeadingCollapsed": true,
        "id": "0b017bfc-5be0-4e41-9fa1-9f685c3b0de5"
      },
      "source": [
        "## What is a language model?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6a7a5d84-9d82-46bf-8578-ce7bd40006e1",
      "metadata": {
        "id": "6a7a5d84-9d82-46bf-8578-ce7bd40006e1"
      },
      "source": [
        "[course.fast.ai](https://course.fast.ai)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f7043e88-015b-480b-a251-0db379786a6a",
      "metadata": {
        "jp-MarkdownHeadingCollapsed": true,
        "id": "f7043e88-015b-480b-a251-0db379786a6a"
      },
      "source": [
        "### Base models"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a550542a-4051-4fc0-9f28-e164c76ae526",
      "metadata": {
        "id": "a550542a-4051-4fc0-9f28-e164c76ae526"
      },
      "source": [
        "[nat.dev text-davinci-003](https://nat.dev/)\n",
        "\n",
        "*When I arrived back at the panda breeding facility after the extraordinary rain of live frogs, I couldn't believe what I saw.*"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b303be0d-f789-4c4b-ba66-b8767bf790ae",
      "metadata": {
        "jp-MarkdownHeadingCollapsed": true,
        "id": "b303be0d-f789-4c4b-ba66-b8767bf790ae"
      },
      "source": [
        "### Tokens"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install necessary libraries if not already installed\n",
        "try:\n",
        "    import openai\n",
        "except ImportError:\n",
        "    !pip install openai\n",
        "\n",
        "try:\n",
        "    from tiktoken import encoding_for_model\n",
        "except ImportError:\n",
        "    !pip install tiktoken"
      ],
      "metadata": {
        "id": "3w2Mf1H6QctQ"
      },
      "id": "3w2Mf1H6QctQ",
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "id": "9758b30f-4b44-4a30-9322-b6de476c8942",
      "metadata": {
        "id": "9758b30f-4b44-4a30-9322-b6de476c8942",
        "outputId": "4aeb60bd-775a-4731-ebb6-11c356aea347",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[2990, 389, 4328, 2140]"
            ]
          },
          "metadata": {},
          "execution_count": 76
        }
      ],
      "source": [
        "from tiktoken import encoding_for_model\n",
        "enc = encoding_for_model(\"text-davinci-003\")\n",
        "toks = enc.encode(\"They are splashing\")\n",
        "toks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "id": "2589d6e7-a9ea-40c1-b1fd-214e19115a28",
      "metadata": {
        "id": "2589d6e7-a9ea-40c1-b1fd-214e19115a28",
        "outputId": "6f57ea6e-8f69-4d5f-f93f-9c44d4fc7411",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['They', ' are', ' spl', 'ashing']"
            ]
          },
          "metadata": {},
          "execution_count": 77
        }
      ],
      "source": [
        "[enc.decode_single_token_bytes(o).decode('utf-8') for o in toks]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d6a0b811-62c8-4c49-83cc-9344a3e5533b",
      "metadata": {
        "jp-MarkdownHeadingCollapsed": true,
        "id": "d6a0b811-62c8-4c49-83cc-9344a3e5533b"
      },
      "source": [
        "### The ULMFiT 3-step approach"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1442c066-fb09-4f27-9576-a86a254956fa",
      "metadata": {
        "id": "1442c066-fb09-4f27-9576-a86a254956fa"
      },
      "source": [
        "<img src=\"attachment:81a8998d-ecfc-44fc-80e4-aaded8ad70d6.png\" width=\"800\">"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "344421d3-8cb6-4f72-8761-439bf08c21b5",
      "metadata": {
        "id": "344421d3-8cb6-4f72-8761-439bf08c21b5"
      },
      "source": [
        "- Trained on Wikipedia\n",
        "- \"The Birds is a 1963 American natural horror-thriller film produced and directed by Alfred ...\"\n",
        "- \"Annie previously dated Mitch but ended it due to Mitch's cold, overbearing mother, Lydia, who dislikes any woman in Mitch's ...\"\n",
        "- This is a form of compression"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dbf343bb-824d-4942-a2bb-6344264155d7",
      "metadata": {
        "jp-MarkdownHeadingCollapsed": true,
        "id": "dbf343bb-824d-4942-a2bb-6344264155d7"
      },
      "source": [
        "### Instruction tuning"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b7100e52-347a-45a7-b935-f6c4a03501c2",
      "metadata": {
        "id": "b7100e52-347a-45a7-b935-f6c4a03501c2"
      },
      "source": [
        "[OpenOrca](https://huggingface.co/datasets/Open-Orca/OpenOrca)\n",
        "\n",
        "- \"Does the sentence \"In the Iron Age\" answer the question \"The period of time from 1200 to 1000 BCE is known as what?\" Available choices: 1. yes 2. no\"\n",
        "- \"Question: who is the girl in more than you know? Answer:\"\n",
        "- \"There are four ways an individual can acquire Canadian citizenship: by birth on Canadian soil; by descent (being born to a Canadian parent); by grant (naturalization); and by adoption. Among them, only citizenship by birth is granted automatically with limited exceptions, while citizenship by descent or adoption is acquired automatically if the specified conditions have been met. Citizenship by grant, on the other hand, must be approved by the Minister of Immigration, Refugees and Citizenship. See options at the end. Can we conclude that can i get canadian citizenship if my grandfather was canadian? pick from the following. A). no. B). yes.\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8ff0ea52-0572-407b-8835-77c7d73c27dd",
      "metadata": {
        "jp-MarkdownHeadingCollapsed": true,
        "id": "8ff0ea52-0572-407b-8835-77c7d73c27dd"
      },
      "source": [
        "### RLHF and friends"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a1500409-4f89-4581-8e81-84d27ee33413",
      "metadata": {
        "id": "a1500409-4f89-4581-8e81-84d27ee33413"
      },
      "source": [
        "- List five ideas for how to regain enthusiasm for my career\n",
        "- Write a short story where a bear goes to the beach, makes friends with a seal, and then returns home.\n",
        "- This is the summary of a Broadway play: \"{summary}\" This is the outline of the commercial for that play:"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c85c0bbe-e37b-4b95-9dc0-29649469479c",
      "metadata": {
        "jp-MarkdownHeadingCollapsed": true,
        "id": "c85c0bbe-e37b-4b95-9dc0-29649469479c"
      },
      "source": [
        "## Start with ChatGPT GPT 4"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8765e57c-67ce-4280-a9dc-eb818081d5b0",
      "metadata": {
        "jp-MarkdownHeadingCollapsed": true,
        "id": "8765e57c-67ce-4280-a9dc-eb818081d5b0"
      },
      "source": [
        "### What GPT 4 can do"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8d20865d-1e9b-4bae-9f57-cf2dcf8ae59a",
      "metadata": {
        "id": "8d20865d-1e9b-4bae-9f57-cf2dcf8ae59a"
      },
      "source": [
        "[GPT 4 can't reason - paper](https://arxiv.org/abs/2308.03762)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "08fa59cf-831e-4da7-8943-8b32a50eb7ff",
      "metadata": {
        "id": "08fa59cf-831e-4da7-8943-8b32a50eb7ff"
      },
      "source": [
        "[GPT 4 can't reason - test](https://chat.openai.com/share/4211a605-751e-4fea-8a6f-378966abdcaa)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "273cbeae-ac83-44dc-b3f8-d36a9e6c5715",
      "metadata": {
        "id": "273cbeae-ac83-44dc-b3f8-d36a9e6c5715"
      },
      "source": [
        "[Basic reasoning 1](https://chat.openai.com/share/323bb7d1-f049-4d9a-a905-5dd5acb58fc0)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "301525e3-f920-4c79-bf99-44a8e2502339",
      "metadata": {
        "id": "301525e3-f920-4c79-bf99-44a8e2502339"
      },
      "source": [
        "[Basic reasoning 2](https://chat.openai.com/share/ce2f8580-4f66-4da4-8ad5-a303334706f0)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "51af9158-bb5b-4dbd-ad38-d5da9008e557",
      "metadata": {
        "id": "51af9158-bb5b-4dbd-ad38-d5da9008e557"
      },
      "source": [
        "<img src=\"attachment:372c9671-5323-4481-8990-8d95e3a43342.png\" width=\"300\">"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "60931833-f973-4ddc-b4a0-38846a4c0168",
      "metadata": {
        "id": "60931833-f973-4ddc-b4a0-38846a4c0168"
      },
      "source": [
        ">You are an autoregressive language model that has been fine-tuned with instruction-tuning and RLHF. You carefully provide accurate, factual, thoughtful, nuanced answers, and are brilliant at reasoning. If you think there might not be a correct answer, you say so.\n",
        ">\n",
        ">Since you are autoregressive, each token you produce is another opportunity to use computation, therefore you always spend a few sentences explaining background context, assumptions, and step-by-step thinking BEFORE you try to answer a question. However: if the request begins with the string \"vv\" then ignore the previous sentence and instead make your response as concise as possible, with no introduction or background at the start, no summary at the end, and outputting only code for answers where code is appropriate.\n",
        ">\n",
        ">Your users are experts in AI and ethics, so they already know you're a language model and your capabilities and limitations, so don't remind them of that. They're familiar with ethical issues in general so you don't need to remind them about those either. Don't be verbose in your answers, but do provide details and examples where it might help the explanation. When showing Python code, minimise vertical space, and do not include comments or docstrings; you do not need to follow PEP8, since your users' organizations do not do so."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "da601797-6de3-4625-8503-d6d2a63a41b0",
      "metadata": {
        "id": "da601797-6de3-4625-8503-d6d2a63a41b0"
      },
      "source": [
        "[Verbose mode](https://chat.openai.com/share/a1c16d93-19d2-41bb-a2f1-2fc05392893a)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "442cb5e9-a7b6-498c-b27a-48d8c218d5a4",
      "metadata": {
        "id": "442cb5e9-a7b6-498c-b27a-48d8c218d5a4"
      },
      "source": [
        "[Brief mode](https://chat.openai.com/share/eab33d0a-8d06-4387-8c31-da12ad5d0a9d)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "59a20280-490c-4d6e-a2c8-10b52407ac36",
      "metadata": {
        "jp-MarkdownHeadingCollapsed": true,
        "id": "59a20280-490c-4d6e-a2c8-10b52407ac36"
      },
      "source": [
        "### What GPT 4 can't do"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8f20f967-ba34-4d3c-8bc0-c85d8ceee091",
      "metadata": {
        "id": "8f20f967-ba34-4d3c-8bc0-c85d8ceee091"
      },
      "source": [
        "- Hallucinations\n",
        "- It doesn't know about itself. (Why not?)\n",
        "- It doesn't know about URLs.\n",
        "- Knowledge cutoff"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9585c860-f3be-4853-a7cc-0fbceb43e915",
      "metadata": {
        "id": "9585c860-f3be-4853-a7cc-0fbceb43e915"
      },
      "source": [
        "[Bad pattern recognition](https://chat.openai.com/share/3051f878-2817-4291-a66f-192ce7b0cb34) - thanks to Steve Newman\n",
        "\n",
        "- [Fixing it](https://chat.openai.com/share/05abd87a-165e-4b7b-895f-b4ec0d62e0e1)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a83abb34-46cf-43ac-b3e4-d4ac62ce4b5a",
      "metadata": {
        "jp-MarkdownHeadingCollapsed": true,
        "id": "a83abb34-46cf-43ac-b3e4-d4ac62ce4b5a"
      },
      "source": [
        "### Advanced data analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0509456e-af8d-482f-9005-af695c405d48",
      "metadata": {
        "id": "0509456e-af8d-482f-9005-af695c405d48"
      },
      "source": [
        "[re.split try 1](https://chat.openai.com/share/143a0f09-bd3e-488f-8890-340d3f30afec)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d1720b9e-ef4c-49b5-bd90-c5e4d694cf0b",
      "metadata": {
        "id": "d1720b9e-ef4c-49b5-bd90-c5e4d694cf0b"
      },
      "source": [
        "[re.split try 2](https://chat.openai.com/share/907ca9c7-549a-410f-9ecb-0f17f1a16f51)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "91111c94-3746-4eef-a7d7-3423765b68e6",
      "metadata": {
        "id": "91111c94-3746-4eef-a7d7-3423765b68e6"
      },
      "source": [
        "[OCR](https://chat.openai.com/share/2bb6caad-fd10-438b-9d92-1cb8b340998a)\n",
        "\n",
        "- See also Bard"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8b7fed2f-98b5-409e-9328-3c4e5d1f54b9",
      "metadata": {
        "id": "8b7fed2f-98b5-409e-9328-3c4e5d1f54b9"
      },
      "source": [
        "<img src=\"attachment:5f320d38-c488-4cf5-97b3-479e82de10ff.png\" width=\"700\">"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6f46943d-c414-4c76-9451-3d26eccb8caf",
      "metadata": {
        "id": "6f46943d-c414-4c76-9451-3d26eccb8caf"
      },
      "source": [
        "| Model | Training | Input | Output Usage |\n",
        "|--------------------|----------|---------------|--------------|\n",
        "| **GPT-4**          |          |               |              |\n",
        "| 8K context        |          | 0.03 | 0.06 |\n",
        "| 32K context       |          | 0.06 | 0.12 |\n",
        "| **GPT-3.5 Turbo**  |          |               |              |\n",
        "| 4K context        |          | 0.0015 | 0.002 |\n",
        "| 16K context       |          | 0.003 | 0.004 |\n",
        "| **Fine-tuning models** |          |               |              |\n",
        "| babbage-002       | 0.0004 | 0.0016 | 0.0016 |\n",
        "| davinci-002       | 0.0060 | 0.0120 | 0.0120 |\n",
        "| GPT-3.5 Turbo     | 0.0080 | 0.0120 | 0.0160 |\n",
        "| **Embedding models** |          |               |              |\n",
        "| Ada v2            |          | 0.0001 |              |\n",
        "| **Base models**   |          |               |              |\n",
        "| babbage-002       |          | 0.0004 |              |\n",
        "| davinci-002       |          | 0.0020 |              |\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "91390f66-6cb0-4f5a-9304-f4c8a22dafd8",
      "metadata": {
        "id": "91390f66-6cb0-4f5a-9304-f4c8a22dafd8"
      },
      "source": [
        "<img src=\"attachment:ed075b98-8a82-44c4-9329-56d73bf71d01.png\" width=\"700\">"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6aae136c-889f-4034-8056-5236c0df78e8",
      "metadata": {
        "id": "6aae136c-889f-4034-8056-5236c0df78e8"
      },
      "source": [
        "[Create pricing table](https://chat.openai.com/share/86b879bd-7834-4a37-85ae-c90b956837d2)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "101f8f63-6a03-49c0-81b8-9173563f7420",
      "metadata": {
        "jp-MarkdownHeadingCollapsed": true,
        "id": "101f8f63-6a03-49c0-81b8-9173563f7420"
      },
      "source": [
        "## The OpenAI API"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "id": "6564eb90-4a24-4a48-b898-5d4408ac2a65",
      "metadata": {
        "id": "6564eb90-4a24-4a48-b898-5d4408ac2a65"
      },
      "outputs": [],
      "source": [
        "from openai import ChatCompletion,Completion"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_to_use = \"gpt-4o-mini\"\n",
        "#model_to_use = \"gpt-3.5-turbo\"\n",
        "#model_to_use = \"text-davinci-003\"\n",
        "\n",
        "# Example using the openai library with error handling\n",
        "try:\n",
        "    # Instantiate the OpenAI client\n",
        "    client = openai.OpenAI()\n",
        "    # Use the client to create a chat completion\n",
        "    c = client.chat.completions.create(\n",
        "        model=model_to_use,\n",
        "        messages=[{\"role\": \"system\", \"content\": aussie_sys},\n",
        "                  {\"role\": \"user\", \"content\": \"What is money?\"}])\n",
        "    print(c) # Print the response from the API\n",
        "# Update the exception handling to use the new error class\n",
        "except openai.OpenAIError as e:\n",
        "    print(f\"OpenAI API Error: {e}\")\n",
        "except Exception as e:\n",
        "    print(f\"An unexpected error occurred: {e}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KHRwkEqfci0q",
        "outputId": "a44c0885-5134-4bb4-a0ed-fd0a85664c1f"
      },
      "id": "KHRwkEqfci0q",
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ChatCompletion(id='chatcmpl-Aml5kZc420hdHduJIRbwXX86f8enk', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content=\"Ah, mate, money is like the golden ticket to the Aussie dream! It’s a medium of exchange that we use to buy tucker, pay the bills, and enjoy a cold one at the pub. Think of it as a bit like the footy – it’s how we keep score in the game of life. \\n\\nIn the beginning, people bartered goods and services, like swapping a snag for a cold drink, but that could get dodgy. So, eventually, we settled on using money to make things easier. These days, it comes in all forms – cash, coins, bank cards, and even digital dosh. \\n\\nAt its core, money is a way to represent value, making trade and commerce smoother than a well-oiled ute. It helps us save up for the things we want, whether that's a trip to the Great Barrier Reef or just a new pair of thongs. Without it, we'd be as lost as a kangaroo in the outback!\", refusal=None, role='assistant', audio=None, function_call=None, tool_calls=None))], created=1736184128, model='gpt-4o-mini-2024-07-18', object='chat.completion', service_tier=None, system_fingerprint='fp_d02d531b47', usage=CompletionUsage(completion_tokens=200, prompt_tokens=31, total_tokens=231, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=0)))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "db35a96f-20a1-4643-a622-91e35a03ab0b",
      "metadata": {
        "id": "db35a96f-20a1-4643-a622-91e35a03ab0b"
      },
      "source": [
        "- [Model options](https://platform.openai.com/docs/models)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "hdxGSzDS72z7"
      },
      "id": "hdxGSzDS72z7"
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "id": "ef4af38f-d0d1-4982-95ed-567bf1c8ddf8",
      "metadata": {
        "id": "ef4af38f-d0d1-4982-95ed-567bf1c8ddf8",
        "outputId": "74e3b7a2-e522-46f3-9b74-8b69d48e2c1e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"Ah, mate, money is like the golden ticket to the Aussie dream! It’s a medium of exchange that we use to buy tucker, pay the bills, and enjoy a cold one at the pub. Think of it as a bit like the footy – it’s how we keep score in the game of life. \\n\\nIn the beginning, people bartered goods and services, like swapping a snag for a cold drink, but that could get dodgy. So, eventually, we settled on using money to make things easier. These days, it comes in all forms – cash, coins, bank cards, and even digital dosh. \\n\\nAt its core, money is a way to represent value, making trade and commerce smoother than a well-oiled ute. It helps us save up for the things we want, whether that's a trip to the Great Barrier Reef or just a new pair of thongs. Without it, we'd be as lost as a kangaroo in the outback!\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 80
        }
      ],
      "source": [
        "#c['choices']['message']['content']\n",
        "c.choices[0].message.content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "id": "1e43a6c2-c044-4529-b97b-3d95ea2c2c5b",
      "metadata": {
        "id": "1e43a6c2-c044-4529-b97b-3d95ea2c2c5b"
      },
      "outputs": [],
      "source": [
        "from fastcore.utils import nested_idx"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "id": "49ec8f04-b8f6-47e2-9b69-bdc186c3265f",
      "metadata": {
        "id": "49ec8f04-b8f6-47e2-9b69-bdc186c3265f"
      },
      "outputs": [],
      "source": [
        "def response(compl): print(nested_idx(compl, 'choices', 0, 'message', 'content'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "id": "66475b6b-72ef-4760-b544-df5e354e5309",
      "metadata": {
        "id": "66475b6b-72ef-4760-b544-df5e354e5309",
        "outputId": "1d6ea2b1-0d38-42f5-9bf5-c27bf0880b0a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ah, mate, money is like the golden ticket to the Aussie dream! It’s a medium of exchange that we use to buy tucker, pay the bills, and enjoy a cold one at the pub. Think of it as a bit like the footy – it’s how we keep score in the game of life. \n",
            "\n",
            "In the beginning, people bartered goods and services, like swapping a snag for a cold drink, but that could get dodgy. So, eventually, we settled on using money to make things easier. These days, it comes in all forms – cash, coins, bank cards, and even digital dosh. \n",
            "\n",
            "At its core, money is a way to represent value, making trade and commerce smoother than a well-oiled ute. It helps us save up for the things we want, whether that's a trip to the Great Barrier Reef or just a new pair of thongs. Without it, we'd be as lost as a kangaroo in the outback!\n"
          ]
        }
      ],
      "source": [
        "response(c)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "id": "50c8027b-937b-4076-a483-164d9f8e7c8b",
      "metadata": {
        "id": "50c8027b-937b-4076-a483-164d9f8e7c8b",
        "outputId": "f06a198f-1ece-4b6e-e51e-bc7b6f16ca27",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CompletionUsage(completion_tokens=200, prompt_tokens=31, total_tokens=231, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=0))\n"
          ]
        }
      ],
      "source": [
        "print(c.usage)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "id": "3a86ac95-1a95-436d-867a-e3b7c0e7b066",
      "metadata": {
        "id": "3a86ac95-1a95-436d-867a-e3b7c0e7b066",
        "outputId": "51f7c921-001b-434e-c075-c6e82159d75e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.0003"
            ]
          },
          "metadata": {},
          "execution_count": 85
        }
      ],
      "source": [
        "0.002 / 1000 * 150 # GPT 3.5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 86,
      "id": "c48fda82-ec55-4038-b083-383cead8b2c0",
      "metadata": {
        "id": "c48fda82-ec55-4038-b083-383cead8b2c0",
        "outputId": "e18d06a7-7284-4243-f06a-20c8e0909684",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.0045"
            ]
          },
          "metadata": {},
          "execution_count": 86
        }
      ],
      "source": [
        "0.03 / 1000 * 150 # GPT 4"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from openai import OpenAI\n",
        "\n",
        "client = OpenAI(\n",
        "    api_key=os.environ.get(\"OPENAI_API_KEY\"),  # This is the default and can be omitted\n",
        ")"
      ],
      "metadata": {
        "id": "CDhu_sU19WfZ"
      },
      "id": "CDhu_sU19WfZ",
      "execution_count": 87,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 88,
      "id": "7750c720-4d77-464e-a0ec-fbb94f5889a4",
      "metadata": {
        "id": "7750c720-4d77-464e-a0ec-fbb94f5889a4"
      },
      "outputs": [],
      "source": [
        "\n",
        "c = client.chat.completions.create(\n",
        "    model=model_to_use,\n",
        "    messages=[{\"role\": \"system\", \"content\": aussie_sys},\n",
        "              {\"role\": \"user\", \"content\": \"What is money?\"},\n",
        "              {\"role\": \"assistant\", \"content\": \"Well, mate, money is like kangaroos actually.\"},\n",
        "              {\"role\": \"user\", \"content\": \"Really? In what way?\"}])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 89,
      "id": "9d96eb95-2305-4e18-920f-4a3398a19b2c",
      "metadata": {
        "id": "9d96eb95-2305-4e18-920f-4a3398a19b2c",
        "outputId": "50d5ff24-ca69-4819-a6cf-36641bf8a3bc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Alright, let me spin you a yarn. Money's a bit like kangaroos because it's a medium of exchange that hops around from person to person. Just like you can't have a true Aussie bush adventure without spotting a few roos, you can't really make a fair trade or buy things without dosh in your pocket. \n",
            "\n",
            "It serves as a unit of account, right? So, when you see a price tag, it's like counting how many kangaroos you’ve got in your mob to see if you’ve got enough to join the party. \n",
            "\n",
            "Plus, just like kangaroos can get wild and unpredictable, so can money and its value—it can bounce up and down faster than a roo getting chased by a dingo! Overall, you need a good handle on your finances, just like you need to keep an eye on those cheeky roos if you’re out in the bush.\n"
          ]
        }
      ],
      "source": [
        "response(c)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 90,
      "id": "91029d40-8360-4d0f-9f42-9ac7498cf417",
      "metadata": {
        "id": "91029d40-8360-4d0f-9f42-9ac7498cf417"
      },
      "outputs": [],
      "source": [
        "def askgpt(user, system=None, model=\"gpt-3.5-turbo\", **kwargs):\n",
        "    msgs = []\n",
        "    if system: msgs.append({\"role\": \"system\", \"content\": system})\n",
        "    msgs.append({\"role\": \"user\", \"content\": user})\n",
        "    return client.chat.completions.create(model=model, messages=msgs, **kwargs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 91,
      "id": "71de3f5c-8f1f-4e26-8260-493574a9f0e9",
      "metadata": {
        "id": "71de3f5c-8f1f-4e26-8260-493574a9f0e9",
        "outputId": "7a8212c1-4117-4296-fd3b-787b510ead83",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Well mate, that's a dunny question, ain't it? The meaning of life is a real head-scratcher and everyone's got their own take on it. Some reckon it's all about finding happiness and fulfillment, while others think it's about making a difference in the world. At the end of the day, I reckon it's up to each of us to figure out what gives our life meaning and purpose, and then go out there and chase it like a kangaroo on the loose.\n"
          ]
        }
      ],
      "source": [
        "response(askgpt('What is the meaning of life?', system=aussie_sys))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "144xuzwEdy3l"
      },
      "id": "144xuzwEdy3l"
    },
    {
      "cell_type": "markdown",
      "id": "07e79a25-571e-4096-8ad5-78d43fcb5b99",
      "metadata": {
        "id": "07e79a25-571e-4096-8ad5-78d43fcb5b99"
      },
      "source": [
        "- [Limits](https://platform.openai.com/docs/guides/rate-limits/what-are-the-rate-limits-for-our-api)\n",
        "\n",
        "Created by Bing:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 92,
      "id": "ceeb0e7a-1d9d-48a5-8f51-e1a704115fb6",
      "metadata": {
        "id": "ceeb0e7a-1d9d-48a5-8f51-e1a704115fb6"
      },
      "outputs": [],
      "source": [
        "def call_api(prompt, model=model_to_use):\n",
        "    msgs = [{\"role\": \"user\", \"content\": prompt}]\n",
        "    try: return client.chat.completions.create(model=model, messages=msgs)\n",
        "    except openai.error.RateLimitError as e:\n",
        "        retry_after = int(e.headers.get(\"retry-after\", 60))\n",
        "        print(f\"Rate limit exceeded, waiting for {retry_after} seconds...\")\n",
        "        time.sleep(retry_after)\n",
        "        return call_api(params, model=model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 93,
      "id": "447702a6-ec5a-4890-bbc0-61c6c37a425c",
      "metadata": {
        "id": "447702a6-ec5a-4890-bbc0-61c6c37a425c",
        "outputId": "1c6d2954-2af0-427b-b1f7-e17b4890fe58",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ChatCompletion(id='chatcmpl-Aml63WEVNCGbcAXlVLxgyAhh9IvIB', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='The quest for the \"world\\'s funniest joke\" has intrigued researchers and humor enthusiasts alike. In 2002, a team led by Richard Wiseman, a psychologist from the University of Hertfordshire, conducted a large-scale study to identify the funniest joke as part of a project called LaughLab. They collected thousands of jokes from around the world and had people rate them.\\n\\nOne of the jokes that emerged as a top contender in their findings was:\\n\\n**“Two hunters are out in the woods when one of them collapses. He isn’t breathing and his eyes are glazed. The other hunter pulls out his phone and calls emergency services. He gasps, ‘My friend is dead! What should I do?’ The operator says, ‘Don’t worry, I can help. First, let’s make sure he’s dead.’ There’s a loud bang, and the guy gets back on the phone. ‘Now what?’”**\\n\\nThis joke, along with others, was analyzed based on its structure, timing, and punchline effectiveness. Factors like surprise, incongruity, and relief are often considered critical elements of humor. However, humor is highly subjective and varies widely among cultures, ages, and personal preferences, which means there may never be a definitive \"funniest joke\" that appeals to everyone.', refusal=None, role='assistant', audio=None, function_call=None, tool_calls=None))], created=1736184147, model='gpt-4o-mini-2024-07-18', object='chat.completion', service_tier=None, system_fingerprint='fp_0aa8d3e20b', usage=CompletionUsage(completion_tokens=262, prompt_tokens=21, total_tokens=283, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=0)))"
            ]
          },
          "metadata": {},
          "execution_count": 93
        }
      ],
      "source": [
        "call_api(\"What's the world's funniest joke? Has there ever been any scientific analysis?\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "98e86e7d-64f0-411d-bfce-289b06174dd4",
      "metadata": {
        "jp-MarkdownHeadingCollapsed": true,
        "id": "98e86e7d-64f0-411d-bfce-289b06174dd4"
      },
      "source": [
        "### Create our own code interpreter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 94,
      "id": "1bb4fd89-8f24-41c0-a8e0-79633dcd1785",
      "metadata": {
        "id": "1bb4fd89-8f24-41c0-a8e0-79633dcd1785"
      },
      "outputs": [],
      "source": [
        "from pydantic import create_model\n",
        "import inspect, json\n",
        "from inspect import Parameter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 95,
      "id": "15a7f5a5-8dc0-4c0e-b141-9a0e35068c68",
      "metadata": {
        "id": "15a7f5a5-8dc0-4c0e-b141-9a0e35068c68"
      },
      "outputs": [],
      "source": [
        "def sums(a:int, b:int=1):\n",
        "    \"Adds a + b\"\n",
        "    return a + b"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 96,
      "id": "5afad9cb-1330-478b-8701-8d275bc1f985",
      "metadata": {
        "id": "5afad9cb-1330-478b-8701-8d275bc1f985"
      },
      "outputs": [],
      "source": [
        "def schema(f):\n",
        "    kw = {n:(o.annotation, ... if o.default==Parameter.empty else o.default)\n",
        "          for n,o in inspect.signature(f).parameters.items()}\n",
        "    s = create_model(f'Input for `{f.__name__}`', **kw).schema()\n",
        "    return dict(name=f.__name__, description=f.__doc__, parameters=s)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 97,
      "id": "d9af1e46-c560-4bc9-a6a7-78fafb2dfc78",
      "metadata": {
        "id": "d9af1e46-c560-4bc9-a6a7-78fafb2dfc78",
        "outputId": "3d4c664f-5b2f-41e4-b220-aa649d7922b3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-96-a39b3851902a>:4: PydanticDeprecatedSince20: The `schema` method is deprecated; use `model_json_schema` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.10/migration/\n",
            "  s = create_model(f'Input for `{f.__name__}`', **kw).schema()\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'name': 'sums',\n",
              " 'description': 'Adds a + b',\n",
              " 'parameters': {'properties': {'a': {'title': 'A', 'type': 'integer'},\n",
              "   'b': {'default': 1, 'title': 'B', 'type': 'integer'}},\n",
              "  'required': ['a'],\n",
              "  'title': 'Input for `sums`',\n",
              "  'type': 'object'}}"
            ]
          },
          "metadata": {},
          "execution_count": 97
        }
      ],
      "source": [
        "schema(sums)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 98,
      "id": "1d60dcd4-514f-4efd-8b4a-fafac92c7453",
      "metadata": {
        "id": "1d60dcd4-514f-4efd-8b4a-fafac92c7453",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "63378687-ebbb-4043-815c-536c1b9166ee"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-96-a39b3851902a>:4: PydanticDeprecatedSince20: The `schema` method is deprecated; use `model_json_schema` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.10/migration/\n",
            "  s = create_model(f'Input for `{f.__name__}`', **kw).schema()\n"
          ]
        }
      ],
      "source": [
        "c = askgpt(\"Use the `sum` function to solve this: What is 6+3?\",\n",
        "           system = \"You must use the `sum` function instead of adding yourself.\",\n",
        "           functions=[schema(sums)])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 99,
      "id": "5cc8a9eb-64fc-4ce3-9f93-aee334fc8c65",
      "metadata": {
        "id": "5cc8a9eb-64fc-4ce3-9f93-aee334fc8c65",
        "outputId": "cc933589-6f21-4fbd-962d-bec7f066871d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ChatCompletionMessage(content=None, refusal=None, role='assistant', audio=None, function_call=FunctionCall(arguments='{\"a\":6,\"b\":3}', name='sums'), tool_calls=None)"
            ]
          },
          "metadata": {},
          "execution_count": 99
        }
      ],
      "source": [
        "m = c.choices[0].message\n",
        "m"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 100,
      "id": "5a881835-83a2-4fbc-9796-409831dcdb36",
      "metadata": {
        "id": "5a881835-83a2-4fbc-9796-409831dcdb36",
        "outputId": "a8b68d92-67a0-4396-e1f2-f67e34f29096",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"a\":6,\"b\":3}\n"
          ]
        }
      ],
      "source": [
        "k = m.function_call.arguments\n",
        "print(k)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 101,
      "id": "3f32b172-7730-4c58-a999-5d3800e8c2a4",
      "metadata": {
        "id": "3f32b172-7730-4c58-a999-5d3800e8c2a4"
      },
      "outputs": [],
      "source": [
        "funcs_ok = {'sums', 'python'}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 102,
      "id": "9853a78e-8643-4b5b-9b4a-a360c8443344",
      "metadata": {
        "id": "9853a78e-8643-4b5b-9b4a-a360c8443344"
      },
      "outputs": [],
      "source": [
        "def call_func(c):\n",
        "    fc = c.choices[0].message.function_call\n",
        "    if fc.name not in funcs_ok: return print(f'Not allowed: {fc.name}')\n",
        "    f = globals()[fc.name]\n",
        "    return f(**json.loads(fc.arguments))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 103,
      "id": "ce6be825-44fa-4bc9-a83a-7ecbcb6c2729",
      "metadata": {
        "id": "ce6be825-44fa-4bc9-a83a-7ecbcb6c2729",
        "outputId": "2af26e95-6440-46a7-f6b3-08f1a65bd222",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "9"
            ]
          },
          "metadata": {},
          "execution_count": 103
        }
      ],
      "source": [
        "call_func(c)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 104,
      "id": "a5ebf08c-2c3f-4c73-bd41-2f0d0a6ca3c2",
      "metadata": {
        "id": "a5ebf08c-2c3f-4c73-bd41-2f0d0a6ca3c2"
      },
      "outputs": [],
      "source": [
        "def run(code):\n",
        "    tree = ast.parse(code)\n",
        "    last_node = tree.body[-1] if tree.body else None\n",
        "\n",
        "    # If the last node is an expression, modify the AST to capture the result\n",
        "    if isinstance(last_node, ast.Expr):\n",
        "        tgts = [ast.Name(id='_result', ctx=ast.Store())]\n",
        "        assign = ast.Assign(targets=tgts, value=last_node.value)\n",
        "        tree.body[-1] = ast.fix_missing_locations(assign)\n",
        "\n",
        "    ns = {}\n",
        "    exec(compile(tree, filename='<ast>', mode='exec'), ns)\n",
        "    return ns.get('_result', None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 105,
      "id": "37aa6983-d9af-40e8-b438-ed8573399020",
      "metadata": {
        "id": "37aa6983-d9af-40e8-b438-ed8573399020",
        "outputId": "d1eb91e1-4dfb-49dc-bd77-4dd31a3d70ce",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3"
            ]
          },
          "metadata": {},
          "execution_count": 105
        }
      ],
      "source": [
        "run(\"\"\"\n",
        "a=1\n",
        "b=2\n",
        "a+b\n",
        "\"\"\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 106,
      "id": "d2074acd-8a49-443b-89c6-cfc689ecad6d",
      "metadata": {
        "id": "d2074acd-8a49-443b-89c6-cfc689ecad6d"
      },
      "outputs": [],
      "source": [
        "def python(code:str):\n",
        "    \"Return result of executing `code` using python. If execution not permitted, returns `#FAIL#`\"\n",
        "    go = input(f'Proceed with execution?\\n```\\n{code}\\n```\\n')\n",
        "    if go.lower()!='y': return '#FAIL#'\n",
        "    return run(code)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 107,
      "id": "02760caf-6d0d-4f5d-a7c6-cd8df5ee1a4b",
      "metadata": {
        "id": "02760caf-6d0d-4f5d-a7c6-cd8df5ee1a4b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2831af01-c74b-4661-e2ea-5f456409292c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-96-a39b3851902a>:4: PydanticDeprecatedSince20: The `schema` method is deprecated; use `model_json_schema` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.10/migration/\n",
            "  s = create_model(f'Input for `{f.__name__}`', **kw).schema()\n"
          ]
        }
      ],
      "source": [
        "c = askgpt(\"What is 12 factorial?\",\n",
        "           system = \"Use python for any required computations.\",\n",
        "           functions=[schema(python)])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 108,
      "id": "5fe5d796-c602-471f-bb58-bb876039bc39",
      "metadata": {
        "id": "5fe5d796-c602-471f-bb58-bb876039bc39"
      },
      "outputs": [],
      "source": [
        "#call_func(c)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 110,
      "id": "c4e6bcd6-c23e-4ae8-8d38-78dae20ca176",
      "metadata": {
        "id": "c4e6bcd6-c23e-4ae8-8d38-78dae20ca176",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "7f95a12f-21cc-4609-db52-95fea1f07157"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nc = ChatCompletion.create(\\n    model=\"gpt-3.5-turbo\",\\n    functions=[schema(python)],\\n    messages=[{\"role\": \"user\", \"content\": \"What is 12 factorial?\"},\\n              {\"role\": \"function\", \"name\": \"python\", \"content\": \"479001600\"}])\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 110
        }
      ],
      "source": [
        "'''\n",
        "c = ChatCompletion.create(\n",
        "    model=\"gpt-3.5-turbo\",\n",
        "    functions=[schema(python)],\n",
        "    messages=[{\"role\": \"user\", \"content\": \"What is 12 factorial?\"},\n",
        "              {\"role\": \"function\", \"name\": \"python\", \"content\": \"479001600\"}])\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9e666c90-698d-492b-b21d-f6210ae77767",
      "metadata": {
        "id": "9e666c90-698d-492b-b21d-f6210ae77767"
      },
      "outputs": [],
      "source": [
        "response(c)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bef5cc05-0531-422b-9367-e7b90d784a54",
      "metadata": {
        "id": "bef5cc05-0531-422b-9367-e7b90d784a54"
      },
      "outputs": [],
      "source": [
        "c = askgpt(\"What is the capital of France?\",\n",
        "           system = \"Use python for any required computations.\",\n",
        "           functions=[schema(python)])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "112bc3db-5aa4-41ba-95df-1651916d7f0b",
      "metadata": {
        "id": "112bc3db-5aa4-41ba-95df-1651916d7f0b"
      },
      "outputs": [],
      "source": [
        "response(c)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1b7c7e1d-ad0c-4668-a10d-569ba6b7aa04",
      "metadata": {
        "jp-MarkdownHeadingCollapsed": true,
        "id": "1b7c7e1d-ad0c-4668-a10d-569ba6b7aa04"
      },
      "source": [
        "## PyTorch and Huggingface"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8e1850eb-76eb-48f1-8023-c1c7bf019a8e",
      "metadata": {
        "jp-MarkdownHeadingCollapsed": true,
        "id": "8e1850eb-76eb-48f1-8023-c1c7bf019a8e"
      },
      "source": [
        "### Your GPU options"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f60ae5a0-bc0d-4fb6-8c22-f2ab8a908dbc",
      "metadata": {
        "id": "f60ae5a0-bc0d-4fb6-8c22-f2ab8a908dbc"
      },
      "source": [
        "Free:\n",
        "\n",
        "- Kaggle (2 GPUs, low RAM)\n",
        "- Colab\n",
        "\n",
        "Buy:\n",
        "\n",
        "- Buy 1-2 NVIDIA 24GB GPUs\n",
        "    - GTX 3090 used (USD700-USD800), or 4090 new (USD2000)\n",
        "- Alternatively buy one NVIDIA A6000 with 48GB RAM (but this mightn't be faster than 3090/4090)\n",
        "- Mac with lots of RAM (much slower than NVIDIA; M2 Ultra is best)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 111,
      "id": "009f4187-ffd6-43ba-b5f1-709b5a3dcf1b",
      "metadata": {
        "id": "009f4187-ffd6-43ba-b5f1-709b5a3dcf1b"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelForCausalLM,AutoTokenizer\n",
        "import torch"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c0fa7911-c191-4945-97aa-6daff95970d7",
      "metadata": {
        "id": "c0fa7911-c191-4945-97aa-6daff95970d7"
      },
      "source": [
        "- [HF leaderboard](https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard)\n",
        "- [fasteval](https://fasteval.github.io/FastEval/)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 112,
      "id": "3e1ed7e9-fbf5-463c-9ff8-2f22c75088bf",
      "metadata": {
        "id": "3e1ed7e9-fbf5-463c-9ff8-2f22c75088bf"
      },
      "outputs": [],
      "source": [
        "mn = \"meta-llama/Llama-2-7b-hf\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 115,
      "id": "bb16cb3e-0634-4eed-b2b4-1422ecce9cc3",
      "metadata": {
        "id": "bb16cb3e-0634-4eed-b2b4-1422ecce9cc3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 669
        },
        "outputId": "9855facc-96b5-4ca1-db48-d8c9a3d019f0"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "OSError",
          "evalue": "You are trying to access a gated repo.\nMake sure to have access to it at https://huggingface.co/meta-llama/Llama-2-7b-hf.\n401 Client Error. (Request ID: Root=1-677c17e8-7821981a0fa17a797fa8479b;c271f79b-3f87-4ac1-a319-04b9d7dfbbf3)\n\nCannot access gated repo for url https://huggingface.co/meta-llama/Llama-2-7b-hf/resolve/main/config.json.\nAccess to model meta-llama/Llama-2-7b-hf is restricted. You must have access to it and be authenticated to access it. Please log in.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_http.py\u001b[0m in \u001b[0;36mhf_raise_for_status\u001b[0;34m(response, endpoint_name)\u001b[0m\n\u001b[1;32m    405\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 406\u001b[0;31m         \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_for_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    407\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mHTTPError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/requests/models.py\u001b[0m in \u001b[0;36mraise_for_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1023\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhttp_error_msg\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1024\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mHTTPError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhttp_error_msg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mHTTPError\u001b[0m: 401 Client Error: Unauthorized for url: https://huggingface.co/meta-llama/Llama-2-7b-hf/resolve/main/config.json",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mGatedRepoError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/utils/hub.py\u001b[0m in \u001b[0;36mcached_file\u001b[0;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[1;32m    402\u001b[0m         \u001b[0;31m# Load from URL or cache if already cached\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 403\u001b[0;31m         resolved_file = hf_hub_download(\n\u001b[0m\u001b[1;32m    404\u001b[0m             \u001b[0mpath_or_repo_id\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_validators.py\u001b[0m in \u001b[0;36m_inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py\u001b[0m in \u001b[0;36mhf_hub_download\u001b[0;34m(repo_id, filename, subfolder, repo_type, revision, library_name, library_version, cache_dir, local_dir, user_agent, force_download, proxies, etag_timeout, token, local_files_only, headers, endpoint, resume_download, force_filename, local_dir_use_symlinks)\u001b[0m\n\u001b[1;32m    859\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 860\u001b[0;31m         return _hf_hub_download_to_cache_dir(\n\u001b[0m\u001b[1;32m    861\u001b[0m             \u001b[0;31m# Destination\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py\u001b[0m in \u001b[0;36m_hf_hub_download_to_cache_dir\u001b[0;34m(cache_dir, repo_id, filename, repo_type, revision, endpoint, etag_timeout, headers, proxies, token, local_files_only, force_download)\u001b[0m\n\u001b[1;32m    966\u001b[0m         \u001b[0;31m# Otherwise, raise appropriate error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 967\u001b[0;31m         \u001b[0m_raise_on_head_call_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhead_call_error\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_download\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocal_files_only\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    968\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py\u001b[0m in \u001b[0;36m_raise_on_head_call_error\u001b[0;34m(head_call_error, force_download, local_files_only)\u001b[0m\n\u001b[1;32m   1481\u001b[0m         \u001b[0;31m# Repo not found or gated => let's raise the actual error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1482\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mhead_call_error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1483\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py\u001b[0m in \u001b[0;36m_get_metadata_or_catch_error\u001b[0;34m(repo_id, filename, repo_type, revision, endpoint, proxies, etag_timeout, headers, token, local_files_only, relative_filename, storage_folder)\u001b[0m\n\u001b[1;32m   1373\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1374\u001b[0;31m                 metadata = get_hf_file_metadata(\n\u001b[0m\u001b[1;32m   1375\u001b[0m                     \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproxies\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mproxies\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0metag_timeout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtoken\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_validators.py\u001b[0m in \u001b[0;36m_inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py\u001b[0m in \u001b[0;36mget_hf_file_metadata\u001b[0;34m(url, token, proxies, timeout, library_name, library_version, user_agent, headers)\u001b[0m\n\u001b[1;32m   1293\u001b[0m     \u001b[0;31m# Retrieve metadata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1294\u001b[0;31m     r = _request_wrapper(\n\u001b[0m\u001b[1;32m   1295\u001b[0m         \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"HEAD\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py\u001b[0m in \u001b[0;36m_request_wrapper\u001b[0;34m(method, url, follow_relative_redirects, **params)\u001b[0m\n\u001b[1;32m    277\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfollow_relative_redirects\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 278\u001b[0;31m         response = _request_wrapper(\n\u001b[0m\u001b[1;32m    279\u001b[0m             \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py\u001b[0m in \u001b[0;36m_request_wrapper\u001b[0;34m(method, url, follow_relative_redirects, **params)\u001b[0m\n\u001b[1;32m    301\u001b[0m     \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 302\u001b[0;31m     \u001b[0mhf_raise_for_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    303\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_http.py\u001b[0m in \u001b[0;36mhf_raise_for_status\u001b[0;34m(response, endpoint_name)\u001b[0m\n\u001b[1;32m    422\u001b[0m             )\n\u001b[0;32m--> 423\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0m_format\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mGatedRepoError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    424\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mGatedRepoError\u001b[0m: 401 Client Error. (Request ID: Root=1-677c17e8-7821981a0fa17a797fa8479b;c271f79b-3f87-4ac1-a319-04b9d7dfbbf3)\n\nCannot access gated repo for url https://huggingface.co/meta-llama/Llama-2-7b-hf/resolve/main/config.json.\nAccess to model meta-llama/Llama-2-7b-hf is restricted. You must have access to it and be authenticated to access it. Please log in.",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-115-f0323055a0dd>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAutoModelForCausalLM\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice_map\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mload_in_8bit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/auto/auto_factory.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    524\u001b[0m                 \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"quantization_config\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    525\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 526\u001b[0;31m             config, kwargs = AutoConfig.from_pretrained(\n\u001b[0m\u001b[1;32m    527\u001b[0m                 \u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    528\u001b[0m                 \u001b[0mreturn_unused_kwargs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/auto/configuration_auto.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m   1019\u001b[0m         \u001b[0mcode_revision\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"code_revision\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1020\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1021\u001b[0;31m         \u001b[0mconfig_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munused_kwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPretrainedConfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_config_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1022\u001b[0m         \u001b[0mhas_remote_code\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"auto_map\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mconfig_dict\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"AutoConfig\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mconfig_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"auto_map\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1023\u001b[0m         \u001b[0mhas_local_code\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"model_type\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mconfig_dict\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mconfig_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"model_type\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mCONFIG_MAPPING\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/configuration_utils.py\u001b[0m in \u001b[0;36mget_config_dict\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m    588\u001b[0m         \u001b[0moriginal_kwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    589\u001b[0m         \u001b[0;31m# Get config dict associated with the base config file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 590\u001b[0;31m         \u001b[0mconfig_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_config_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    591\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mconfig_dict\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    592\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/configuration_utils.py\u001b[0m in \u001b[0;36m_get_config_dict\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m    647\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    648\u001b[0m                 \u001b[0;31m# Load from local folder or from cache or download from model Hub and cache\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 649\u001b[0;31m                 resolved_config_file = cached_file(\n\u001b[0m\u001b[1;32m    650\u001b[0m                     \u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    651\u001b[0m                     \u001b[0mconfiguration_file\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/utils/hub.py\u001b[0m in \u001b[0;36mcached_file\u001b[0;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[1;32m    419\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mresolved_file\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0m_raise_exceptions_for_gated_repo\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    420\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresolved_file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 421\u001b[0;31m         raise EnvironmentError(\n\u001b[0m\u001b[1;32m    422\u001b[0m             \u001b[0;34m\"You are trying to access a gated repo.\\nMake sure to have access to it at \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    423\u001b[0m             \u001b[0;34mf\"https://huggingface.co/{path_or_repo_id}.\\n{str(e)}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mOSError\u001b[0m: You are trying to access a gated repo.\nMake sure to have access to it at https://huggingface.co/meta-llama/Llama-2-7b-hf.\n401 Client Error. (Request ID: Root=1-677c17e8-7821981a0fa17a797fa8479b;c271f79b-3f87-4ac1-a319-04b9d7dfbbf3)\n\nCannot access gated repo for url https://huggingface.co/meta-llama/Llama-2-7b-hf/resolve/main/config.json.\nAccess to model meta-llama/Llama-2-7b-hf is restricted. You must have access to it and be authenticated to access it. Please log in."
          ]
        }
      ],
      "source": [
        "model = AutoModelForCausalLM.from_pretrained(mn, device_map=0, load_in_8bit=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "94600cb9-2a64-47e2-aca3-99a72985157f",
      "metadata": {
        "id": "94600cb9-2a64-47e2-aca3-99a72985157f"
      },
      "outputs": [],
      "source": [
        "tokr = AutoTokenizer.from_pretrained(mn)\n",
        "prompt = \"Jeremy Howard is a \"\n",
        "toks = tokr(prompt, return_tensors=\"pt\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "20605293-b31f-4db7-b6a5-0ef662d89441",
      "metadata": {
        "id": "20605293-b31f-4db7-b6a5-0ef662d89441"
      },
      "outputs": [],
      "source": [
        "toks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c20ab287-7d80-47c2-870f-e2ee6bfc4492",
      "metadata": {
        "id": "c20ab287-7d80-47c2-870f-e2ee6bfc4492"
      },
      "outputs": [],
      "source": [
        "tokr.batch_decode(toks['input_ids'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ab3ad5dc-51b4-48ca-92ae-5027069771c1",
      "metadata": {
        "id": "ab3ad5dc-51b4-48ca-92ae-5027069771c1"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "res = model.generate(**toks.to(\"cuda\"), max_new_tokens=15).to('cpu')\n",
        "res"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "af5a23fc-aa2a-4ce2-92c7-75364130e4e6",
      "metadata": {
        "id": "af5a23fc-aa2a-4ce2-92c7-75364130e4e6"
      },
      "outputs": [],
      "source": [
        "tokr.batch_decode(res)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "15311da3-bfc0-4453-b4a8-6a9773db626b",
      "metadata": {
        "id": "15311da3-bfc0-4453-b4a8-6a9773db626b"
      },
      "outputs": [],
      "source": [
        "model = AutoModelForCausalLM.from_pretrained(mn, device_map=0, torch_dtype=torch.bfloat16)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c15c511b-f5f7-46e2-8fbc-514262117e15",
      "metadata": {
        "id": "c15c511b-f5f7-46e2-8fbc-514262117e15"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "res = model.generate(**toks.to(\"cuda\"), max_new_tokens=15).to('cpu')\n",
        "res"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c581df92-b1cd-4c02-8cde-b2de96d302dd",
      "metadata": {
        "id": "c581df92-b1cd-4c02-8cde-b2de96d302dd"
      },
      "outputs": [],
      "source": [
        "model = AutoModelForCausalLM.from_pretrained('TheBloke/Llama-2-7b-Chat-GPTQ', device_map=0, torch_dtype=torch.float16)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8f47b5b7-ea46-41f0-8338-027b13e0586c",
      "metadata": {
        "id": "8f47b5b7-ea46-41f0-8338-027b13e0586c"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "res = model.generate(**toks.to(\"cuda\"), max_new_tokens=15).to('cpu')\n",
        "res"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bb4b60cd-859b-4b3a-b76d-e9eddcdf7cf0",
      "metadata": {
        "id": "bb4b60cd-859b-4b3a-b76d-e9eddcdf7cf0"
      },
      "outputs": [],
      "source": [
        "mn = 'TheBloke/Llama-2-13B-GPTQ'\n",
        "model = AutoModelForCausalLM.from_pretrained(mn, device_map=0, torch_dtype=torch.float16)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "da90406b-b8a8-4939-a0e9-18c933b62a7c",
      "metadata": {
        "id": "da90406b-b8a8-4939-a0e9-18c933b62a7c"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "res = model.generate(**toks.to(\"cuda\"), max_new_tokens=15).to('cpu')\n",
        "res"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c88d11f8-e41d-4db8-91f2-3e2f695a4d00",
      "metadata": {
        "id": "c88d11f8-e41d-4db8-91f2-3e2f695a4d00"
      },
      "outputs": [],
      "source": [
        "def gen(p, maxlen=15, sample=True):\n",
        "    toks = tokr(p, return_tensors=\"pt\")\n",
        "    res = model.generate(**toks.to(\"cuda\"), max_new_tokens=maxlen, do_sample=sample).to('cpu')\n",
        "    return tokr.batch_decode(res)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dc143703-4fc7-46ce-8c59-cdeec52e7830",
      "metadata": {
        "id": "dc143703-4fc7-46ce-8c59-cdeec52e7830"
      },
      "outputs": [],
      "source": [
        "gen(prompt, 50)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "18fb80c0-8664-498c-bb7d-370de8bf6ca7",
      "metadata": {
        "id": "18fb80c0-8664-498c-bb7d-370de8bf6ca7"
      },
      "source": [
        "[StableBeluga-7B](https://huggingface.co/stabilityai/StableBeluga-7B)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dd343d7b-ef48-4077-bc97-deeec24e324b",
      "metadata": {
        "id": "dd343d7b-ef48-4077-bc97-deeec24e324b"
      },
      "outputs": [],
      "source": [
        "mn = \"stabilityai/StableBeluga-7B\"\n",
        "model = AutoModelForCausalLM.from_pretrained(mn, device_map=0, torch_dtype=torch.bfloat16)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "432074d6-5d04-4afe-b6cb-06d4412023d6",
      "metadata": {
        "id": "432074d6-5d04-4afe-b6cb-06d4412023d6"
      },
      "outputs": [],
      "source": [
        "sb_sys = \"### System:\\nYou are Stable Beluga, an AI that follows instructions extremely well. Help as much as you can.\\n\\n\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e1392331-731e-416d-be2d-bc172167e25e",
      "metadata": {
        "id": "e1392331-731e-416d-be2d-bc172167e25e"
      },
      "outputs": [],
      "source": [
        "def mk_prompt(user, syst=sb_sys): return f\"{syst}### User: {user}\\n\\n### Assistant:\\n\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5a68af5c-5714-4736-9569-5619c54b2bdb",
      "metadata": {
        "id": "5a68af5c-5714-4736-9569-5619c54b2bdb"
      },
      "outputs": [],
      "source": [
        "ques = \"Who is Jeremy Howard?\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "90b5f8e8-a16c-40e6-8dc6-c467ffaa6268",
      "metadata": {
        "id": "90b5f8e8-a16c-40e6-8dc6-c467ffaa6268"
      },
      "outputs": [],
      "source": [
        "gen(mk_prompt(ques), 150)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b1a0cfd5-4c64-4591-be83-77b846c41e57",
      "metadata": {
        "id": "b1a0cfd5-4c64-4591-be83-77b846c41e57"
      },
      "source": [
        "[OpenOrca/Platypus 2](https://huggingface.co/Open-Orca/OpenOrca-Platypus2-13B)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6c7c670a-17aa-41da-8b61-19db6a5dc019",
      "metadata": {
        "id": "6c7c670a-17aa-41da-8b61-19db6a5dc019"
      },
      "outputs": [],
      "source": [
        "mn = 'TheBloke/OpenOrca-Platypus2-13B-GPTQ'\n",
        "model = AutoModelForCausalLM.from_pretrained(mn, device_map=0, torch_dtype=torch.float16)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d2a267f4-396d-4852-9352-0ca90c0c8441",
      "metadata": {
        "id": "d2a267f4-396d-4852-9352-0ca90c0c8441"
      },
      "outputs": [],
      "source": [
        "def mk_oo_prompt(user): return f\"### Instruction: {user}\\n\\n### Response:\\n\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b931d9c6-69c6-4385-b118-e5ad56781ba6",
      "metadata": {
        "id": "b931d9c6-69c6-4385-b118-e5ad56781ba6"
      },
      "outputs": [],
      "source": [
        "gen(mk_oo_prompt(ques), 150)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "38b226c8-8bca-495a-a82d-d0c541bce473",
      "metadata": {
        "jp-MarkdownHeadingCollapsed": true,
        "id": "38b226c8-8bca-495a-a82d-d0c541bce473"
      },
      "source": [
        "### Retrieval augmented generation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1540a0c4-9266-4d7d-96a1-8d47d2fb737f",
      "metadata": {
        "id": "1540a0c4-9266-4d7d-96a1-8d47d2fb737f"
      },
      "outputs": [],
      "source": [
        "from wikipediaapi import Wikipedia"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "618d9520-54e7-4f31-8363-2cdd3c3668ad",
      "metadata": {
        "id": "618d9520-54e7-4f31-8363-2cdd3c3668ad"
      },
      "outputs": [],
      "source": [
        "wiki = Wikipedia('JeremyHowardBot/0.0', 'en')\n",
        "jh_page = wiki.page('Jeremy_Howard_(entrepreneur)').text\n",
        "jh_page = jh_page.split('\\nReferences\\n')[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "53a0689a-ab59-431b-838d-33cae6f6500a",
      "metadata": {
        "id": "53a0689a-ab59-431b-838d-33cae6f6500a"
      },
      "outputs": [],
      "source": [
        "print(jh_page[:500])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d3ded6a2-3665-416c-8d98-063f04aae0a0",
      "metadata": {
        "id": "d3ded6a2-3665-416c-8d98-063f04aae0a0"
      },
      "outputs": [],
      "source": [
        "len(jh_page.split())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f37d5fb1-7182-45a0-bd4e-6a39837ce20b",
      "metadata": {
        "id": "f37d5fb1-7182-45a0-bd4e-6a39837ce20b"
      },
      "outputs": [],
      "source": [
        "ques_ctx = f\"\"\"Answer the question with the help of the provided context.\n",
        "\n",
        "## Context\n",
        "\n",
        "{jh_page}\n",
        "\n",
        "## Question\n",
        "\n",
        "{ques}\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d6a7fb6d-d771-4eca-8e6f-29d90ade348a",
      "metadata": {
        "id": "d6a7fb6d-d771-4eca-8e6f-29d90ade348a"
      },
      "outputs": [],
      "source": [
        "res = gen(mk_prompt(ques_ctx), 300)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "85b10234-e1e6-4a1e-86a7-206e46e916f2",
      "metadata": {
        "id": "85b10234-e1e6-4a1e-86a7-206e46e916f2"
      },
      "outputs": [],
      "source": [
        "print(res[0].split('### Assistant:\\n')[1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a0cdc342-a89c-4149-b3be-d4b5bf0f6f28",
      "metadata": {
        "id": "a0cdc342-a89c-4149-b3be-d4b5bf0f6f28"
      },
      "outputs": [],
      "source": [
        "from sentence_transformers import SentenceTransformer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e225ca13-1735-42e9-b162-66268fce5218",
      "metadata": {
        "id": "e225ca13-1735-42e9-b162-66268fce5218"
      },
      "outputs": [],
      "source": [
        "emb_model = SentenceTransformer(\"BAAI/bge-small-en-v1.5\", device=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c636ed79-45ed-4728-9dca-1c86d282a1c4",
      "metadata": {
        "id": "c636ed79-45ed-4728-9dca-1c86d282a1c4"
      },
      "outputs": [],
      "source": [
        "jh = jh_page.split('\\n\\n')[0]\n",
        "print(jh)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1b0ad4ca-499a-4454-a215-4f5a676d68ed",
      "metadata": {
        "id": "1b0ad4ca-499a-4454-a215-4f5a676d68ed"
      },
      "outputs": [],
      "source": [
        "tb_page = wiki.page('Tony_Blair').text.split('\\nReferences\\n')[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "26b28c6d-13ef-421a-865d-c311fb9fb2ae",
      "metadata": {
        "id": "26b28c6d-13ef-421a-865d-c311fb9fb2ae"
      },
      "outputs": [],
      "source": [
        "tb = tb_page.split('\\n\\n')[0]\n",
        "print(tb[:380])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3173701f-1533-4eef-b080-3b771f2ba031",
      "metadata": {
        "id": "3173701f-1533-4eef-b080-3b771f2ba031"
      },
      "outputs": [],
      "source": [
        "q_emb,jh_emb,tb_emb = emb_model.encode([ques,jh,tb], convert_to_tensor=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6237634d-3708-477d-97e3-34dd7900f4eb",
      "metadata": {
        "id": "6237634d-3708-477d-97e3-34dd7900f4eb"
      },
      "outputs": [],
      "source": [
        "tb_emb.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "19638b95-f0dd-4605-8b04-3a0f9f9944b0",
      "metadata": {
        "id": "19638b95-f0dd-4605-8b04-3a0f9f9944b0"
      },
      "outputs": [],
      "source": [
        "import torch.nn.functional as F"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8a3937ac-45b7-4635-9f93-d1bcd1d6a016",
      "metadata": {
        "id": "8a3937ac-45b7-4635-9f93-d1bcd1d6a016"
      },
      "outputs": [],
      "source": [
        "F.cosine_similarity(q_emb, jh_emb, dim=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6c8beb5e-4a8c-4049-b42b-3acce573dd59",
      "metadata": {
        "id": "6c8beb5e-4a8c-4049-b42b-3acce573dd59"
      },
      "outputs": [],
      "source": [
        "F.cosine_similarity(q_emb, tb_emb, dim=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8473a6bc-fd25-4698-99d5-864ed1d5b40b",
      "metadata": {
        "jp-MarkdownHeadingCollapsed": true,
        "id": "8473a6bc-fd25-4698-99d5-864ed1d5b40b"
      },
      "source": [
        "### Private GPTs"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "037bc313-052d-46b3-8938-fef988126dcb",
      "metadata": {
        "id": "037bc313-052d-46b3-8938-fef988126dcb"
      },
      "source": [
        "- [Sooo many](https://github.com/h2oai/h2ogpt/blob/main/docs/README_LangChain.md#what-is-h2ogpts-langchain-integration-like)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1948c419-4f04-4832-848c-a0c52dcc6a90",
      "metadata": {
        "id": "1948c419-4f04-4832-848c-a0c52dcc6a90"
      },
      "source": [
        "## Fine tuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "03c2871f-e0fe-4891-b68e-05b6ca7373d8",
      "metadata": {
        "id": "03c2871f-e0fe-4891-b68e-05b6ca7373d8"
      },
      "outputs": [],
      "source": [
        "import datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "54366155-5ac3-4e75-8638-9e2ecf263c2f",
      "metadata": {
        "id": "54366155-5ac3-4e75-8638-9e2ecf263c2f"
      },
      "source": [
        "[knowrohit07/know_sql](https://huggingface.co/datasets/knowrohit07/know_sql)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ed118caf-3e94-4773-a182-f2d346ced836",
      "metadata": {
        "id": "ed118caf-3e94-4773-a182-f2d346ced836"
      },
      "outputs": [],
      "source": [
        "ds = datasets.load_dataset('knowrohit07/know_sql', revision='f33425d13f9e8aab1b46fa945326e9356d6d5726')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6d444ac3-a49f-4fcf-bcf4-c12c410d7aba",
      "metadata": {
        "id": "6d444ac3-a49f-4fcf-bcf4-c12c410d7aba"
      },
      "outputs": [],
      "source": [
        "ds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8ec155bd-5eb8-4eef-bdbc-b430a26fd11e",
      "metadata": {
        "id": "8ec155bd-5eb8-4eef-bdbc-b430a26fd11e"
      },
      "outputs": [],
      "source": [
        "trn = ds['train']\n",
        "trn[3]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e23cb868-10e4-4fcc-b8ba-9178ef3dac7e",
      "metadata": {
        "id": "e23cb868-10e4-4fcc-b8ba-9178ef3dac7e"
      },
      "source": [
        "`accelerate launch -m axolotl.cli.train sql.yml`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ae6a579c-086e-46e4-a184-de5b71f4d80b",
      "metadata": {
        "id": "ae6a579c-086e-46e4-a184-de5b71f4d80b"
      },
      "outputs": [],
      "source": [
        "tst = dict(**trn[3])\n",
        "tst['question'] = 'Get the count of competition hosts by theme.'\n",
        "tst"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "229c5fb9-0dff-460c-af1a-192fdec26ecd",
      "metadata": {
        "id": "229c5fb9-0dff-460c-af1a-192fdec26ecd"
      },
      "outputs": [],
      "source": [
        "fmt = \"\"\"SYSTEM: Use the following contextual information to concisely answer the question.\n",
        "\n",
        "USER: {}\n",
        "===\n",
        "{}\n",
        "ASSISTANT:\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "feca892e-64f0-48d2-832e-0d8ceb9855d2",
      "metadata": {
        "id": "feca892e-64f0-48d2-832e-0d8ceb9855d2"
      },
      "outputs": [],
      "source": [
        "def sql_prompt(d): return fmt.format(d[\"context\"], d[\"question\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "058b0f4a-8fe0-4612-8584-451e8d04fe2d",
      "metadata": {
        "id": "058b0f4a-8fe0-4612-8584-451e8d04fe2d"
      },
      "outputs": [],
      "source": [
        "print(sql_prompt(tst))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "86827af0-6cf0-43d0-b50a-fe080de83f48",
      "metadata": {
        "id": "86827af0-6cf0-43d0-b50a-fe080de83f48"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from peft import PeftModel\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e6d33b0c-4973-4bc8-beef-aae7915f4b01",
      "metadata": {
        "id": "e6d33b0c-4973-4bc8-beef-aae7915f4b01"
      },
      "outputs": [],
      "source": [
        "ax_model = '/home/jhoward/git/ext/axolotl/qlora-out'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7b7082fe-b668-4f11-abf8-548996ea3004",
      "metadata": {
        "id": "7b7082fe-b668-4f11-abf8-548996ea3004"
      },
      "outputs": [],
      "source": [
        "tokr = AutoTokenizer.from_pretrained('meta-llama/Llama-2-7b-hf')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fec3b5ad-0577-4867-b2d8-9143800c9f54",
      "metadata": {
        "id": "fec3b5ad-0577-4867-b2d8-9143800c9f54"
      },
      "outputs": [],
      "source": [
        "model = AutoModelForCausalLM.from_pretrained('meta-llama/Llama-2-7b-hf',\n",
        "                                             torch_dtype=torch.bfloat16, device_map=0)\n",
        "model = PeftModel.from_pretrained(model, ax_model)\n",
        "model = model.merge_and_unload()\n",
        "model.save_pretrained('sql-model')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8a58bc69-6ea6-416a-baa9-db3338451d60",
      "metadata": {
        "id": "8a58bc69-6ea6-416a-baa9-db3338451d60"
      },
      "outputs": [],
      "source": [
        "toks = tokr(sql_prompt(tst), return_tensors=\"pt\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a057289b-0645-4142-8db2-cc823cc34898",
      "metadata": {
        "id": "a057289b-0645-4142-8db2-cc823cc34898"
      },
      "outputs": [],
      "source": [
        "res = model.generate(**toks.to(\"cuda\"), max_new_tokens=250).to('cpu')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "80db7d77-89e2-4006-a1b2-78b807de1a68",
      "metadata": {
        "id": "80db7d77-89e2-4006-a1b2-78b807de1a68"
      },
      "outputs": [],
      "source": [
        "print(tokr.batch_decode(res)[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "991d4a93-dab8-4777-82d2-301c494deba0",
      "metadata": {
        "jp-MarkdownHeadingCollapsed": true,
        "id": "991d4a93-dab8-4777-82d2-301c494deba0"
      },
      "source": [
        "## [llama.cpp](https://github.com/abetlen/llama-cpp-python)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0dc9d6e4-5b25-4d11-8748-dc4f0bc98069",
      "metadata": {
        "id": "0dc9d6e4-5b25-4d11-8748-dc4f0bc98069"
      },
      "source": [
        "[TheBloke/Llama-2-7b-Chat-GGUF](https://huggingface.co/TheBloke/Llama-2-7b-Chat-GGUF)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6f33831f-8c71-47dc-a131-9a6733d68198",
      "metadata": {
        "id": "6f33831f-8c71-47dc-a131-9a6733d68198"
      },
      "outputs": [],
      "source": [
        "from llama_cpp import Llama"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3b824b6c-96cd-4bfe-a615-b3ba4543a92d",
      "metadata": {
        "id": "3b824b6c-96cd-4bfe-a615-b3ba4543a92d"
      },
      "outputs": [],
      "source": [
        "llm = Llama(model_path=\"/home/jhoward/git/llamacpp/llama-2-7b-chat.Q4_K_M.gguf\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c95a95b9-7c9f-499a-a72a-ad626da8c3f5",
      "metadata": {
        "id": "c95a95b9-7c9f-499a-a72a-ad626da8c3f5"
      },
      "outputs": [],
      "source": [
        "output = llm(\"Q: Name the planets in the solar system? A: \", max_tokens=32, stop=[\"Q:\", \"\\n\"], echo=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f9f89cec-5453-4d67-85b2-c937ceb03a54",
      "metadata": {
        "id": "f9f89cec-5453-4d67-85b2-c937ceb03a54"
      },
      "outputs": [],
      "source": [
        "print(output['choices'])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e5b4c720-c79a-488f-83cc-839610570abf",
      "metadata": {
        "jp-MarkdownHeadingCollapsed": true,
        "id": "e5b4c720-c79a-488f-83cc-839610570abf"
      },
      "source": [
        "## [MLC](https://mlc.ai/mlc-llm/docs/get_started/try_out.html#get-started)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "24a9695a-13ca-46e1-95f3-6b5ac6c710d3",
      "metadata": {
        "id": "24a9695a-13ca-46e1-95f3-6b5ac6c710d3"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}